{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfe56af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tpot\n",
    "from tpot.tpot import TPOTClustering\n",
    "import pandas as pd\n",
    "import neptune.new as neptune\n",
    "\n",
    "run_times = \"5\"\n",
    "\n",
    "api_token = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIwODNjNDRiNS02MDM4LTQ2NGEtYWQwMC00OGRhYjcwODc0ZDIifQ==\"\n",
    "project_name = \"MaleLab/TPOT4ClusteringLight\"\n",
    "run_id = \"LIGHTTPOT-3631\"\n",
    "run = neptune.init_run(\n",
    "    project=project_name, with_id=run_id, api_token=api_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95394bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run[\"mo\"].fetch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500b7b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = run[\"dataset\"].fetch()\n",
    "mo = run[\"mo\"].fetch()\n",
    "gen = run[\"gp/gen\"].fetch()\n",
    "pop = run[\"gp/pop\"].fetch()\n",
    "run_id = run[\"sys/id\"].fetch()\n",
    "run_number = run[\"status\"].fetch()\n",
    "bic = run[\"scorers/bic\"].fetch()\n",
    "chs = run[\"scorers/chs\"].fetch()\n",
    "dbs = run[\"scorers/dbs\"].fetch()\n",
    "sil = run[\"scorers/sil\"].fetch()\n",
    "dataset = pd.read_csv(f\"datasets/{dataset_name}.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488010d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sil != \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8768c821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== TPOT CLUSTERING ==================== \n",
      " Run ID: TEST - Dataset: a1 - Scorers: ['sil', 'dbs', 'chs'] - mo: mean_score\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "590a0db8168e4d4eaa972c8372e60b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/60 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting: Pipeline(steps=[('meanshift', MeanShift(max_iter=400, min_bin_freq=3))])\n",
      "Fitting: Pipeline(steps=[('birch',\n",
      "                 Birch(branching_factor=10, n_clusters=17, threshold=0.001))])\n",
      "Fitting: Pipeline(steps=[('kmeans', KMeans(init='random', max_iter=500, n_clusters=15))])\n",
      "Fitting: Pipeline(steps=[('kmeans',\n",
      "                 KMeans(algorithm='elkan', max_iter=100, n_clusters=15))])\n",
      "Fitting: Pipeline(steps=[('normalizer', Normalizer()),\n",
      "                ('spectralclustering',\n",
      "                 SpectralClustering(affinity='nearest_neighbors',\n",
      "                                    eigen_solver='arpack', n_clusters=9))])\n",
      "Fitting: Pipeline(steps=[('dbscan',\n",
      "                 DBSCAN(eps=10.0, leaf_size=35, metric='l1', min_samples=25))])\n",
      "Fitting: Pipeline(steps=[('variancethreshold', VarianceThreshold(threshold=0.1)),\n",
      "                ('dbscan',\n",
      "                 DBSCAN(eps=0.1, leaf_size=15, metric='manhattan',\n",
      "                        min_samples=3))])\n",
      "Fitting: Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n",
      "                ('kmeans',\n",
      "                 KMeans(algorithm='elkan', max_iter=100, n_clusters=12))])\n",
      "Fitting: Pipeline(steps=[('birch',\n",
      "                 Birch(branching_factor=10, n_clusters=19, threshold=0.001))])\n",
      "Fitting: Pipeline(steps=[('bisectingkmeans',\n",
      "                 BisectingKMeans(bisecting_strategy='largest_cluster',\n",
      "                                 init='k-means++', max_iter=400,\n",
      "                                 n_clusters=16))])\n",
      "Population: MeanShift(input_matrix, MeanShift__cluster_all=True, MeanShift__max_iter=400, MeanShift__min_bin_freq=3)\n",
      "Population: Birch(input_matrix, Birch__branching_factor=10, Birch__n_clusters=17, Birch__threshold=0.001)\n",
      "Population: KMeans(input_matrix, KMeans__algorithm=lloyd, KMeans__init=random, KMeans__max_iter=500, KMeans__n_clusters=15)\n",
      "Population: KMeans(input_matrix, KMeans__algorithm=elkan, KMeans__init=k-means++, KMeans__max_iter=100, KMeans__n_clusters=15)\n",
      "Population: SpectralClustering(Normalizer(input_matrix, Normalizer__norm=l2), SpectralClustering__affinity=nearest_neighbors, SpectralClustering__eigen_solver=arpack, SpectralClustering__n_clusters=9)\n",
      "Population: DBSCAN(input_matrix, DBSCAN__eps=10.0, DBSCAN__leaf_size=35, DBSCAN__metric=l1, DBSCAN__min_samples=25)\n",
      "Population: DBSCAN(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.1), DBSCAN__eps=0.1, DBSCAN__leaf_size=15, DBSCAN__metric=manhattan, DBSCAN__min_samples=3)\n",
      "Population: KMeans(MinMaxScaler(input_matrix), KMeans__algorithm=elkan, KMeans__init=k-means++, KMeans__max_iter=100, KMeans__n_clusters=12)\n",
      "Population: Birch(input_matrix, Birch__branching_factor=10, Birch__n_clusters=19, Birch__threshold=0.001)\n",
      "Population: BisectingKMeans(input_matrix, BisectingKMeans__algorithm=lloyd, BisectingKMeans__bisecting_strategy=largest_cluster, BisectingKMeans__init=k-means++, BisectingKMeans__max_iter=400, BisectingKMeans__n_clusters=16)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "A pipeline has not yet been optimized. Please call fit() first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/dev/tpot_dev/tpot/base.py:824\u001b[0m, in \u001b[0;36mTPOTBase.fit\u001b[0;34m(self, features, target, sample_weight, groups, mo_function, scorers)\u001b[0m\n\u001b[1;32m    823\u001b[0m         warnings\u001b[39m.\u001b[39msimplefilter(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 824\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pop, _ \u001b[39m=\u001b[39m eaMuPlusLambda(\n\u001b[1;32m    825\u001b[0m             population\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pop,\n\u001b[1;32m    826\u001b[0m             toolbox\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_toolbox,\n\u001b[1;32m    827\u001b[0m             mu\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpopulation_size,\n\u001b[1;32m    828\u001b[0m             lambda_\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lambda,\n\u001b[1;32m    829\u001b[0m             cxpb\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcrossover_rate,\n\u001b[1;32m    830\u001b[0m             mutpb\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmutation_rate,\n\u001b[1;32m    831\u001b[0m             ngen\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerations,\n\u001b[1;32m    832\u001b[0m             pbar\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pbar,\n\u001b[1;32m    833\u001b[0m             halloffame\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pareto_front,\n\u001b[1;32m    834\u001b[0m             verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbosity,\n\u001b[1;32m    835\u001b[0m             per_generation_function\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_periodic_pipeline,\n\u001b[1;32m    836\u001b[0m             log_file\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog_file_,\n\u001b[1;32m    837\u001b[0m         )\n\u001b[1;32m    839\u001b[0m \u001b[39m# Allow for certain exceptions to signal a premature fit() cancellation\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/tpot_dev/tpot/gp_deap.py:238\u001b[0m, in \u001b[0;36meaMuPlusLambda\u001b[0;34m(population, toolbox, mu, lambda_, cxpb, mutpb, ngen, pbar, stats, halloffame, verbose, per_generation_function, log_file)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39m# print(\"\\n>>> Populacao Inicial:\\n\")\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[39m# [print(f\"Individual: {pipeline}\") for pipeline in population]\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[39m# print(\"\\n\")\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m population[:] \u001b[39m=\u001b[39m toolbox\u001b[39m.\u001b[39;49mevaluate(population)\n\u001b[1;32m    239\u001b[0m record \u001b[39m=\u001b[39m stats\u001b[39m.\u001b[39mcompile(population) \u001b[39mif\u001b[39;00m stats \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n",
      "File \u001b[0;32m~/dev/tpot_dev/tpot/base.py:1724\u001b[0m, in \u001b[0;36mTPOTBase._evaluate_individuals\u001b[0;34m(self, population, features, target, sample_weight, groups, mo_function, scorers)\u001b[0m\n\u001b[1;32m   1723\u001b[0m individuals \u001b[39m=\u001b[39m [ind \u001b[39mfor\u001b[39;00m ind \u001b[39min\u001b[39;00m population \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ind\u001b[39m.\u001b[39mfitness\u001b[39m.\u001b[39mvalid]\n\u001b[0;32m-> 1724\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update(population)\n\u001b[1;32m   1726\u001b[0m \u001b[39mreturn\u001b[39;00m population\n",
      "File \u001b[0;32m~/dev/tpot_dev/tpot/base.py:1487\u001b[0m, in \u001b[0;36mTPOTBase._update\u001b[0;34m(self, population)\u001b[0m\n\u001b[1;32m   1486\u001b[0m [\u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPopulation: \u001b[39m\u001b[39m{\u001b[39;00mindividual\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m individual \u001b[39min\u001b[39;00m population]\n\u001b[0;32m-> 1487\u001b[0m quit()\n\u001b[1;32m   1488\u001b[0m \u001b[39mfor\u001b[39;00m ind \u001b[39min\u001b[39;00m population:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'quit' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     10\u001b[0m clusterer \u001b[39m=\u001b[39m TPOTClustering(\n\u001b[1;32m     11\u001b[0m     generations\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,\n\u001b[1;32m     12\u001b[0m     population_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     n_jobs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m==================== TPOT CLUSTERING ==================== \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Run ID: \u001b[39m\u001b[39m{\u001b[39;00mrun_id\u001b[39m}\u001b[39;00m\u001b[39m - Dataset: \u001b[39m\u001b[39m{\u001b[39;00mdataset_name\u001b[39m}\u001b[39;00m\u001b[39m - Scorers: \u001b[39m\u001b[39m{\u001b[39;00m_scorers\u001b[39m}\u001b[39;00m\u001b[39m - mo: \u001b[39m\u001b[39m{\u001b[39;00mmo\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m clusterer\u001b[39m.\u001b[39;49mfit(dataset, mo_function\u001b[39m=\u001b[39;49mmo, scorers\u001b[39m=\u001b[39;49m_scorers)\n\u001b[1;32m     20\u001b[0m pipeline, scores, clusters \u001b[39m=\u001b[39m clusterer\u001b[39m.\u001b[39mget_run_stats()\n",
      "File \u001b[0;32m~/dev/tpot_dev/tpot/base.py:871\u001b[0m, in \u001b[0;36mTPOTBase.fit\u001b[0;34m(self, features, target, sample_weight, groups, mo_function, scorers)\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mSystemExit\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    869\u001b[0m         \u001b[39m# raise the exception if it's our last attempt\u001b[39;00m\n\u001b[1;32m    870\u001b[0m         \u001b[39mif\u001b[39;00m attempt \u001b[39m==\u001b[39m (attempts \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m--> 871\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    872\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/dev/tpot_dev/tpot/base.py:862\u001b[0m, in \u001b[0;36mTPOTBase.fit\u001b[0;34m(self, features, target, sample_weight, groups, mo_function, scorers)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pbar, \u001b[39mtype\u001b[39m(\u001b[39mNone\u001b[39;00m)):\n\u001b[1;32m    860\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pbar\u001b[39m.\u001b[39mclose()\n\u001b[0;32m--> 862\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_top_pipeline()\n\u001b[1;32m    863\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_summary_of_best_pipeline(features)\n\u001b[1;32m    864\u001b[0m \u001b[39m# Delete the temporary cache before exiting\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/tpot_dev/tpot/base.py:970\u001b[0m, in \u001b[0;36mTPOTBase._update_top_pipeline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    966\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_optimized_pareto_front_n_gens \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    967\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    968\u001b[0m     \u001b[39m# If user passes CTRL+C in initial generation, self._pareto_front (halloffame) shoule be not updated yet.\u001b[39;00m\n\u001b[1;32m    969\u001b[0m     \u001b[39m# need raise RuntimeError because no pipeline has been optimized\u001b[39;00m\n\u001b[0;32m--> 970\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    971\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mA pipeline has not yet been optimized. Please call fit() first.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    972\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: A pipeline has not yet been optimized. Please call fit() first."
     ]
    }
   ],
   "source": [
    "import tpot\n",
    "from tpot.tpot import TPOTClustering\n",
    "import pandas as pd\n",
    "\n",
    "_scorers=[\"sil\",\"dbs\",\"chs\"]\n",
    "run_id = \"TEST\"\n",
    "dataset_name = \"a1\"\n",
    "dataset = pd.read_csv(f\"datasets/{dataset_name}.csv\")\n",
    "mo = \"mean_score\"\n",
    "clusterer = TPOTClustering(\n",
    "    generations=5,\n",
    "    population_size=10,\n",
    "    verbosity=2,\n",
    "    config_dict=tpot.config.clustering_config_dict,\n",
    "    n_jobs=1,\n",
    ")\n",
    "print(f\"\\n==================== TPOT CLUSTERING ==================== \\n Run ID: {run_id} - Dataset: {dataset_name} - Scorers: {_scorers} - mo: {mo}\")\n",
    "clusterer.fit(dataset, mo_function=mo, scorers=_scorers)\n",
    "\n",
    "pipeline, scores, clusters = clusterer.get_run_stats()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8982f78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c55191",
   "metadata": {},
   "outputs": [],
   "source": [
    "run[\"clusters\"] = str(clusters)\n",
    "run[\"pipeline\"] = pipeline\n",
    "run[\"status\"] = int(run[\"status\"].fetch()) + 1\n",
    "for scorer_name in _scorers:\n",
    "    scorer_key = f\"scorers/{scorer_name}\"\n",
    "    run[scorer_key] = float(run[scorer_key].fetch()) + round(scores[scorer_name], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f076c502",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.sync()\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e025392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune.new as neptune\n",
    "\n",
    "api_token = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIwODNjNDRiNS02MDM4LTQ2NGEtYWQwMC00OGRhYjcwODc0ZDIifQ==\"\n",
    "project_name = \"MaleLab/TPOT4ClusteringLight\"\n",
    "project = neptune.init_project(project=project_name, api_token=api_token)\n",
    "run_times = 5\n",
    "columns = [\n",
    "    \"sys/id\",\n",
    "    \"dataset\",\n",
    "    \"clusters\",\n",
    "    \"status\",\n",
    "    \"scorers/sil\",\n",
    "    \"gp/gen\",\n",
    "    \"gp/pop\",\n",
    "    \"mo\",\n",
    "    \"pipeline\",\n",
    "    \"scorers/bic\",\n",
    "    \"scorers/chz\",\n",
    "    \"scorers/dbi\",\n",
    "    \"scorers/sil\",\n",
    "]\n",
    "\n",
    "runs_table_df = project.fetch_runs_table(columns=columns).to_pandas()\n",
    "\n",
    "run = runs_table_df[\n",
    "    (runs_table_df[\"status\"] < run_times) & (runs_table_df[\"scorers/bic\"] == \"None\")\n",
    "].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe9bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e9d3652",
   "metadata": {},
   "source": [
    "### Generate Run Config Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286d2f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "_csv_path = \"tpot_experiments.csv\"\n",
    "_db_version = \"v1.0\"\n",
    "gp_config = (10, 25)\n",
    "run_times = 10\n",
    "# status = [\"active\", \"inactive\", \"error\", \"done\"]\n",
    "datasets_names = [\n",
    "    \"a1\",\n",
    "    \"a2\",\n",
    "    \"a3\",\n",
    "    \"atom\",\n",
    "    \"birch1\",\n",
    "    \"birch2\",\n",
    "    \"birch3\",\n",
    "    \"chainlink\",\n",
    "    \"dim3\",\n",
    "    \"dim4\",\n",
    "    \"dim5\",\n",
    "    \"dim6\",\n",
    "    \"dim7\",\n",
    "    \"dim8\",\n",
    "    \"dim9\",\n",
    "    \"dim10\",\n",
    "    \"dim11\",\n",
    "    \"dim12\",\n",
    "    \"dim13\",\n",
    "    \"dim14\",\n",
    "    \"dim15\",\n",
    "    \"dim032\",\n",
    "    \"dim064\",\n",
    "    \"dim128\",\n",
    "    \"dim256\",\n",
    "    \"dim512\",\n",
    "    \"dim1024\",\n",
    "    \"engytime\",\n",
    "    \"golfball\",\n",
    "    \"hepta\",\n",
    "    \"lsun3d\",\n",
    "    \"s1\",\n",
    "    \"s2\",\n",
    "    \"s3\",\n",
    "    \"s4\",\n",
    "    \"target\",\n",
    "    \"tetra\",\n",
    "    \"twodiamonds\",\n",
    "    \"unbalance\",\n",
    "    \"wingnut\",\n",
    "]\n",
    "\n",
    "mo_functions = [\n",
    "    \"mean_score\",\n",
    "    \"median_score\",\n",
    "    \"euclidean_score\",\n",
    "    \"seuclidean_score\",\n",
    "    \"sqeuclidean_score\",\n",
    "    \"minkowski_score\",\n",
    "    \"gmean_score\",\n",
    "    \"hmean_score\",\n",
    "    # \"n_max_score\",\n",
    "    \"div_score\",\n",
    "    # \"majority_score\",\n",
    "]\n",
    "\n",
    "scorers = [\"sil\", \"chs\", \"dbs\", \"bic\"]\n",
    "\n",
    "def _scorer_comb(scorers: list) -> list:\n",
    "    all_combinations = itertools.chain.from_iterable(\n",
    "        itertools.combinations(scorers, r) for r in range(1, len(scorers) + 1)\n",
    "    )\n",
    "    scorer_combinations = []\n",
    "    for comb in all_combinations:\n",
    "        scorer_combinations.append(comb)\n",
    "    return scorer_combinations\n",
    "\n",
    "\n",
    "def _head():\n",
    "    return {\n",
    "        \"dataset\": [],\n",
    "        \"mo\": [],\n",
    "        \"gen\": [],\n",
    "        \"pop\": [],\n",
    "        \"sil\": [],\n",
    "        \"chs\": [],\n",
    "        \"dbs\": [],\n",
    "        \"bic\": [],\n",
    "        \"status\": [],\n",
    "    }\n",
    "\n",
    "\n",
    "def _experiments_dict(\n",
    "    datasets_names: list, mo_functions: list, scorers: list\n",
    ") -> list:\n",
    "    combination = []\n",
    "    for combo in itertools.product(datasets_names, mo_functions, scorers):\n",
    "        combination.append(combo)\n",
    "    return combination\n",
    "\n",
    "\n",
    "def _generate_df(head: dict, body: list) -> pd.DataFrame:\n",
    "    for dataset, mo, scorers in body:\n",
    "        # head[\"run_id\"].append(f\"tpot_{dataset}_\")\n",
    "        for i in range(0,run_times):\n",
    "            head[\"dataset\"].append(dataset)\n",
    "            head[\"mo\"].append(mo)\n",
    "            head[\"chs\"].append(\"-\")\n",
    "            head[\"bic\"].append(\"-\")\n",
    "            head[\"dbs\"].append(\"-\")\n",
    "            head[\"sil\"].append(\"-\")\n",
    "            head[\"gen\"].append(gp_config[0])\n",
    "            head[\"pop\"].append(gp_config[1])\n",
    "            head[\"status\"].append(\"inactive\")\n",
    "            if \"chs\" in scorers:\n",
    "                head[\"chs\"].pop()\n",
    "                head[\"chs\"].append(0.0)\n",
    "            if \"bic\" in scorers:\n",
    "                head[\"bic\"].pop()\n",
    "                head[\"bic\"].append(0.0)\n",
    "            if \"dbs\" in scorers:\n",
    "                head[\"dbs\"].pop()\n",
    "                head[\"dbs\"].append(0.0)\n",
    "            if \"sil\" in scorers:\n",
    "                head[\"sil\"].pop()\n",
    "                head[\"sil\"].append(0.0)\n",
    "\n",
    "    return pd.DataFrame(data=head)\n",
    "\n",
    "scorers_comb = _scorer_comb(scorers)\n",
    "header = _head()\n",
    "body = _experiments_dict(datasets_names, mo_functions, scorers_comb)\n",
    "df = _generate_df(header, body)\n",
    "df.to_csv(_csv_path, index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1cc443c8",
   "metadata": {},
   "source": [
    "### Mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb1f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819e977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo \n",
    "client = pymongo.MongoClient(\"mongodb+srv://matheuscmilo:2nX9PPTOuePWUWuJ@malelab.d6wbyrk.mongodb.net/?retryWrites=true&w=majority\")\n",
    "db = client.tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5406ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_configs = df.to_dict(\"records\")\n",
    "result = db.runs.insert_many(run_configs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24841794",
   "metadata": {},
   "source": [
    "#### Mongodb api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9859cfb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': '6408abb072183f17e46b012d',\n",
       " 'dataset': 'a1',\n",
       " 'mo': 'mean_score',\n",
       " 'gen': 10,\n",
       " 'pop': 25,\n",
       " 'sil': 0,\n",
       " 'chs': '-',\n",
       " 'dbs': '-',\n",
       " 'bic': '-',\n",
       " 'status': 'inactive'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find\n",
    "import requests\n",
    "import json\n",
    "find_one_url = \"https://eu-central-1.aws.data.mongodb-api.com/app/data-vhbni/endpoint/data/v1/action/findOne\"\n",
    "find_all_url = \"https://eu-central-1.aws.data.mongodb-api.com/app/data-vhbni/endpoint/data/v1/action/find\"\n",
    "\n",
    "payload = json.dumps({\n",
    "    \"collection\": \"runs\",\n",
    "    \"database\": \"tpot\",\n",
    "    \"dataSource\": \"Malelab\",\n",
    "    \"filter\": {\n",
    "      \"_id\": { \"$oid\": \"6408abb072183f17e46b012d\" },\n",
    "      # \"dataset\": \"a1\",\n",
    "      # \"status\": \"active\"\n",
    "    }\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'Access-Control-Request-Headers': '*',\n",
    "  'api-key': 'tURg5ipsw8mFrPwY52B0d37bzsRQLyk1UFOjFz0fkicfra1FzlcrsDwOl4ctCymr', \n",
    "}\n",
    "\n",
    "response = requests.request(\"POST\", find_one_url, headers=headers, data=payload)\n",
    "_response = response.json()\n",
    "_response[\"document\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5225043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(_response[\"document\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19dccf4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"matchedCount\":0,\"modifiedCount\":0}'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#update\n",
    "import requests\n",
    "import json\n",
    "update_one_url = \"https://eu-central-1.aws.data.mongodb-api.com/app/data-vhbni/endpoint/data/v1/action/updateOne\"\n",
    "update_all_url = \"https://eu-central-1.aws.data.mongodb-api.com/app/data-vhbni/endpoint/data/v1/action/updateMany\"\n",
    "\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'Access-Control-Request-Headers': '*',\n",
    "  'api-key': 'tURg5ipsw8mFrPwY52B0d37bzsRQLyk1UFOjFz0fkicfra1FzlcrsDwOl4ctCymr', \n",
    "}\n",
    "\n",
    "payload = json.dumps({\n",
    "    \"collection\": \"runs\",\n",
    "    \"database\": \"tpot\",\n",
    "    \"dataSource\": \"Malelab\",\n",
    "    \"filter\": {\n",
    "      \"dataset\": \"a1\",\n",
    "      \"bic\": \"-\",\n",
    "      \"status\": \"inactive\"\n",
    "    },\n",
    "    \"update\":{\n",
    "        \"$set\": {\n",
    "            \"status\":\"active\"\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "response = requests.request(\"POST\", update_all_url, headers=headers, data=payload)\n",
    "response.text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
